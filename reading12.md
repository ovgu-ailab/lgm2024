---
layout: default
title: Reading 12
id: reading12
---


# Reading Assignment 12: Large Language Models

There are _many_ language models being developed by large companies and research
groups. Most of these function similarly. We will look at a select few only.
As before, this is a _lot_ to read, so papers are mainly included as a reference.

There is a HUGE overview paper with [A Survey of Large Language Models](https://arxiv.org/abs/2303.18223).

## Model Examples

GPT series of papers:
- [GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
- [GPT 2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [GPT 3](https://arxiv.org/pdf/2005.14165.pdf)
- GPT 4
  - [Paper](https://arxiv.org/pdf/2303.12712.pdf)
  - [Technical Report](https://arxiv.org/pdf/2303.08774.pdf)

As these are all developed by OpenAI, you could also check [PaLM](https://arxiv.org/pdf/2204.02311.pdf)
by Google.

Finally, a very important piece of research, providing justification for this
research agenda, is [Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361.pdf).


## Reinforcement Learning with Human Feedback

An important technique in fine-tuning LLMs, especially for human interaction.

- [A paper](https://arxiv.org/pdf/1909.08593.pdf)
- [Another one](https://arxiv.org/pdf/2203.02155.pdf)
- [A blog post](https://huggingface.co/blog/rlhf)
