---
layout: default
title: Reading 11
id: reading11
---


# Reading Assignment 11: Selected Advanced Topics

This reading is intended to cover advanced architectures that we did not have time
for in the respective sessions. Again you cannot be expected to read all these papers,
but try to read some introductions, conclusions, and figures.


## VAEs

- [Neural Discrete Representation Learning](https://arxiv.org/pdf/1711.00937.pdf)
aka VQ-VAE
- [VQ-VAE 2](https://arxiv.org/pdf/1906.00446.pdf) for very impressive autoencoder-based results


## GANs

- [Progressive Growing](https://arxiv.org/pdf/1710.10196.pdf)
- [StyleGAN](https://arxiv.org/pdf/1812.04948.pdf)
- [StyleGAN 2](https://arxiv.org/pdf/1912.04958.pdf)

This is an excellent series of papers, doing a great job at explaining their choices.
They explain architectures as well as all the "little tricks" that really make it
work.


Aside from that, these are also interesting works, but we likely won't have time to cover them:

- [CycleGAN](https://junyanz.github.io/CycleGAN/) for translating between unpaired
data domains

Finally, a series on different GAN losses and the gap between theory and practice:
- [Wasserstein GAN](https://arxiv.org/pdf/1701.07875.pdf)
- [Improved Wasserstein GAN](https://arxiv.org/pdf/1704.00028.pdf)
- [Wasserstein GANs Work Because They Fail](https://arxiv.org/pdf/2103.01678.pdf)


### Other

- [Denoising Diffusion Implicit Models](https://arxiv.org/pdf/2010.02502.pdf) (DDIM)
is a technique commonly employed in advanced diffusion architectures as it speeds
up sampling significantly.
