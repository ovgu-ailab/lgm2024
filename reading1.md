---
layout: default
title: Reading Assignment 1
id: reading1
---


# Reading Assignment 1: Graphical Models & Monte Carlo Methods

This reading assignment covers required fundamentals that are essential for understanding the probabilistic approaches to deep learning we will focus on in this course.

From the [Deep Learning Book - Chapter 16: Structured Probabilistic Models for Deep Learning](http://www.deeplearningbook.org/contents/graphical_models.html), please read these sections:

* 16.1 The Challenge of Unstructured Modeling
  * Motivation for probabilistic (generative) models difference to supervised learning capabilities and use cases specific to generative models

* 16.2.1 - 16.2.5 Using Graphs to Describe Model Structure
  (This is the most important section.)
  * Why do we need a partition function and how does that function relate to directed and undirected models?
  * What is the main advantage of energy-based models?
  * How is (free) energy related to the model’s capability to model data?  
  * What does separation tell us about conditional dependencies?

* 16.3 Sampling from Graphical Models  
  (This is the second most important section.)

* 16.4 Advantages of Structured Modeling

* 16.5 Learning about Dependencies

Sections 16.6 and 16.7 are optional. These topics will be covered later in depth.

Furthermore, please read [Deep Learning Book - Chapter 17: Monte Carlo Methods](http://www.deeplearningbook.org/contents/monte_carlo.html) with special emphasis on these sections:

* 17.1 Sampling and Monte Carlo Methods
	(for introduction)
  * Why do we need to sample?
  * What is the key idea about Monte Carlo?
* 17.2: Importance sampling is optional

* 17.3 Markov Chain Monte Carlo Methods
  * What is MCMC and under which conditions it can be applied?
  * What is ‘burning in’ of markov chains?

* 17.4 Gibbs Sampling
  * How does Gibbs sampling work?

* 17.5 The Challenge of Mixing between Separated Modes
  * What is mixing and how is this related to MCMC methods? (no details)



